# -*- coding: utf-8 -*-
"""merge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SiYG02nDLo45k7zOrOPr967IjhuNWARn
"""

import os
import requests
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

base_url = 'https://raw.githubusercontent.com/nschaetti/SFGram-dataset/master/book-contents/'
file_prefix = 'book'
file_extension = '.txt'
book_numbers = range(1, 1004)
chunk_size = 250
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer('all-mpnet-base-v2', device=device)
print(f"Model loaded to Deivce：{device}")
all_embeddings = []

for book_number in book_numbers:
    file_name = f"{file_prefix}{book_number:05}{file_extension}"
    file_url = os.path.join(base_url, file_name)
    all_paragraphs = []

    try:
        response = requests.get(file_url)
        if response.status_code != 200:
            print(f"Skip:{file_name}")
            continue
        paragraphs = []
        current_paragraph = []

        for line in response.text.split('\n'):
            if line.strip():  
                current_paragraph.append(line.strip())
            elif current_paragraph:  
                paragraphs.append(" ".join(current_paragraph))
                current_paragraph = []

        if current_paragraph:
            paragraphs.append(" ".join(current_paragraph))

        all_paragraphs.extend(paragraphs)

        final_text = " ".join(all_paragraphs)

        
        words = final_text.split()
        chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

        book_embeddings = [model.encode(chunk, device="cuda") for chunk in chunks]

        all_embeddings.extend(book_embeddings)

        print(f"The book{book_number} is processed with embeddings：{len(book_embeddings)}")

    except Exception as e:
        print(f"there is the error with {file_name} :{e}")

all_embeddings_array = np.array(all_embeddings)

output_path = './Evaluation/Rule_based/all_embeddings.npy'
np.save(output_path, all_embeddings_array)

print(f"\n All books embeddings have been load in: '{output_path}', the shape is:{all_embeddings_array.shape}")