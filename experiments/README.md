# Run Experiments

This folder contains the scripts and configuration files necessary to run single-agent and multi-agent frameworks on our SciFi-100 Dataset. <br>

## Prerequisites
Before running the experiment scripts, ensure that the following prerequisites are met:

### 1. Config File
You need a configuration file for the agents. See details in the [llm-dis](multi_agent/agent_roles_discuss_llama.json), [llm-teacher](multi_agent/agent_roles_teacher_llama.json), and [llm-review](multi_agent/agent_roles_review_llama.json)'s agent config files.

### 2. **Llama Agent**
You need to:
- request access to llama model on hugging face `meta-llama/Llama-3.1-8B-Instruct`
- get [hugging face access token](https://huggingface.co/docs/hub/en/security-tokens)
- edit token permissions to allow read access to public gated repos (add meta-llama/Llama-3.1-8B-Instruct in repositories permissions)
- login to huggingface console in command line: `huggingface-cli login`


## Multi-agent Framework

## Usage 

```bash
cd experiments/multi_agent
python llm_creativity.py -c <path_to_config_file> -d <path_to_dataset_file> -t <task_type> [-r <rounds>]
```

#### For example: 
Our LLM Teacher framework with the SciFi dataset:
```bash
python llm_creativity.py -c agent_roles_teacher_llama.json -d ../../datasets/SciFi/scientific_writing.json -r 5 -t SciFi-Teacher
```

Our LLM Review framework with the SciFi dataset:
```bash
python llm_creativity.py -c agent_roles_review_llama.json -d ../../datasets/SciFi/scientific_writing.json -r 5 -t SciFi-Review
```

The baseline LLM Discussion framework with the SciFi dataset:
```bash
python llm_creativity.py -c agent_roles_discuss_llama.json -d ../../datasets/SciFi/scientific_writing.json -r 5 -t SciFi-Dis
```

### Arguments:
- -c, --config: Required. Path to the configuration file for agents.
- -d, --dataset: Required. Path to the dataset file.
- -t, --type: Required. Type of framework to run. Choose from SciFi-Dis, SciFi-Teacher, and Sci-Fi-Review.
- -r, --rounds: Number of rounds in the discussion. Default is 5.

### Output File
The script will <ins>automatically</ins> create the necessary output folders if they do not exist. These folders will be created under the `Results/{task_type}` directory structure (`task_type` is "SciFi-Dis", "SciFi-Teacher", or "SciFi-Review"). The output files will be automatically uploaded to the GCP bucket.

Subfolders for storing different types of data:
- `Results/{task_type}/chat_log`: Contains chat logs of the entire discussion.
- `Results/{task_type}/Output/multi_agent`: Contains the final discussion results.
- `Results/{task_type}/init`: Stores initial responses generated by the agents.


## Single-agent Framework

## Usage

```bash
cd experiments/single_agent
python single_agent.py
```

## Decoding Experiments

Our framework is designed for flexible and exploratory decoding experiments, so we do not include specific experiment code within the framework itself. Instead, we provide comprehensive instructions for conducting these experiments.

The combination of our framework and the Llama Instruct model supports plug-and-play experimentation with decoding hyperparameters such as **top-p**, **top-k**, **repetition penalty**, and **temperature**.

### Steps to Run Decoding Experiments

#### Using `Agent.py`

1. In the `model.generate()` method, comment out the hyperparameter you wish to use for the decoding strategy (e.g., temperature or top-p).
2. The hyperparameter value specified via the positional variable in the `generate_answer()` method will be used for the chosen decoding strategy.

#### Using `Compose.py`

1. Update the output folder name in the `generate_filename()` and `generate_final_filename()` static methods to save your decoding experiment results in a designated folder (e.g., `Results/Decode-Exp/Top-P`).
2. In the `run()` method for each framework, select the **decoding experiment section** instead of the default **normal runs**.
3. Define the list of values you want to test and pass the hyperparameter to the `process_example()` method within the decoding experiment section. This will also facilitate the creation of a randomly sampled dataset from our SciFi-100 Data to evaluate each decoding strategy.
4. Finally, in the `process_example()` method, assign the chosen value to the positional variable in the `agent.generate_answer()` call (e.g., `temperature=temp`).

Following these steps will enable you to configure and run decoding strategies for each agent in each round of your experiments within the framework.

